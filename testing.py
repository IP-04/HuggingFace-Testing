# -*- coding: utf-8 -*-
"""Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/116BbcpXjcMIaCTzp7mORJjOk4tko_pGI

**Installing Git and authenticating GitHub**
"""

!apt install git

!git config --global user.email "isaiasxl21@gmail.com"
!git config --global user.name "IP-04"

"""**Installing Required libraries**

transformers → Pretrained models from Hugging Face.

datasets → Loading IMDB/Yelp datasets easily.

evaluate → Computing Accuracy & F1 Score.

torch → PyTorch for deep learning.

matplotlib → For visualization.

scikit-learn → For extra evaluation metrics.
"""

!pip install transformers datasets evaluate torch matplotlib scikit-learn

"""**Authenticating Hugging Face token:**"""

from huggingface_hub import login

login(token="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")

"""**Step 1: Picking a dataset**

I decided to test out the **IMDB** dataset:
dataset = load_dataset("imdb")

UPDATE: I decided to also add the Yelp dataset

**Loading data:**
"""

import random

random.seed(42)

from datasets import load_dataset

dataset = load_dataset("imdb")
#Updated
yelp_dataset = load_dataset("yelp_review_full")

# Checking the dataset struct
print(dataset)
print(dataset["train"][0])

"""Converting Yelp Dataset to binary for similarity to IMDB:"""

def convert_yelp_to_binary(example):
    #  Let 1-2 stars = negative (0), 4-5 stars = positive (1), 3 stars = excluded

    if example['label'] <= 1:  # 1-2 stars in original dataset are 0-1 in the labels
        return {'binary_label': 0, 'text': example['text']}
    elif example['label'] >= 3:  # 4-5 stars in original dataset are 3-4 in the labels
        return {'binary_label': 1, 'text': example['text']}
    else:
        return {'binary_label': None, 'text': example['text']}  # 3 stars are neutral

# Apply the conversion
yelp_binary = yelp_dataset.map(convert_yelp_to_binary)

# Filtering out the more neutral reviews and creatinga new binary dataset
from datasets import DatasetDict, Dataset

yelp_binary_dataset = DatasetDict({
    'train': Dataset.from_dict({
        'text': [example['text'] for example in yelp_binary['train'] if example['binary_label'] is not None],
        'label': [example['binary_label'] for example in yelp_binary['train'] if example['binary_label'] is not None]
    }),
    'test': Dataset.from_dict({
        'text': [example['text'] for example in yelp_binary['test'] if example['binary_label'] is not None],
        'label': [example['binary_label'] for example in yelp_binary['test'] if example['binary_label'] is not None]
    })
})

#structure
print("\nYelp Binary Dataset:")
print(f"Train size: {len(yelp_binary_dataset['train'])}")
print(f"Test size: {len(yelp_binary_dataset['test'])}")
print(f"Example: {yelp_binary_dataset['train'][0]}")

# Checking the label distribution
yelp_pos = sum(1 for label in yelp_binary_dataset['test']['label'] if label == 1)
yelp_neg = sum(1 for label in yelp_binary_dataset['test']['label'] if label == 0)
print(f"Yelp test set has {yelp_pos} positive and {yelp_neg} negative reviews")

"""**Step 2: Selecting Pretrained Models**

I decided to start out with the first ones provided:

**DistilBERT** (distilbert-base-uncased-finetuned-sst-2-english)

**BERT **(nlptown/bert-base-multilingual-uncased-sentiment)

UPDATE: For fun, I added the third model l (RoBERTa)

Loading models using the Hugging Face pipelines:
"""

from transformers import pipeline

# Loading models
model_1 = pipeline("sentiment-analysis",
                   model="distilbert-base-uncased-finetuned-sst-2-english",
                   tokenizer="distilbert-base-uncased-finetuned-sst-2-english",
                   device=0,   # to use the GPU
                   truncation=True)  # fixed a problem with Truncate texts > 512 tokens
model_2 = pipeline("sentiment-analysis",
                   model="nlptown/bert-base-multilingual-uncased-sentiment",
                   tokenizer="nlptown/bert-base-multilingual-uncased-sentiment",
                   device=0,
                   truncation=True)
#Updated 3 model
model_3 = pipeline("sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                tokenizer="cardiffnlp/twitter-roberta-base-sentiment-latest",
                device=0,
                truncation=True)

# Testing that models loaded correctly:
test_text = "This movie was fantastic! The plot was engaging and the characters were well developed."
print("DistilBERT Prediction:", model_1(test_text))
print("BERT Prediction:", model_2(test_text))

"""**Step 3:  Measuring Performance**

**Creating helper function to evaluate Model Performance on test data:**

(using accuracy and F1 Score)
"""

def evaluate_model(model, dataset, model_name, num_samples=500):
    # Find positive and negative examples
    positive_indices = [i for i, example in enumerate(dataset["test"]) if example["label"] == 1]
    negative_indices = [i for i, example in enumerate(dataset["test"]) if example["label"] == 0]

    print(f"Total positive examples: {len(positive_indices)}")
    print(f"Total negative examples: {len(negative_indices)}")

    # Sample equally from both classes
    samples_per_class = min(num_samples // 2, min(len(positive_indices), len(negative_indices)))

    sampled_pos_indices = random.sample(positive_indices, samples_per_class)
    sampled_neg_indices = random.sample(negative_indices, samples_per_class)
    sampled_indices = sampled_pos_indices + sampled_neg_indices


    random.shuffle(sampled_indices)
    true_labels = []
    predictions = []

    # Debugging
    if len(sampled_indices) > 0:
        sample_text = dataset["test"][sampled_indices[0]]["text"]
        if model_name == "RoBERTa":
            sample_text = sample_text[:200]

        try:
            sample_output = model(sample_text, truncation=True, max_length=100)
            print(f"Sample model output format for {model_name}:", sample_output)
        except Exception as e:
            print(f"Error getting sample output for {model_name}: {e}")

    for i in sampled_indices:
        try:
            text = dataset["test"][i]["text"]
            label = dataset["test"][i]["label"]

            if model_name == "RoBERTa":
                text = text[:200]  # Short text for RoBERTa to avoid errors

            # Prediction with explicit truncation parameters
            pred = model(text, truncation=True, max_length=100)[0]

            # Handle different model output formats
            if model_name == "DistilBERT":
                # DistilBERT: "POSITIVE" or "NEGATIVE"
                pred_label = 1 if pred["label"] == "POSITIVE" else 0
            elif model_name == "BERT":
                # BERT model: star ratings (1-5)
                label_text = pred["label"]
                star_rating = int(label_text.split()[0])
                pred_label = 1 if star_rating >= 3 else 0
            elif model_name == "RoBERTa":
                # RoBERTa model:  "positive", "negative", or "neutral"
                label_text = pred["label"].lower()
                if label_text == "positive":
                    pred_label = 1
                elif label_text == "negative":
                    pred_label = 0
                else:  # neutral case
                    pred_label = label

            true_labels.append(label)
            predictions.append(pred_label)
        except Exception as e:
            continue

    # Double check distribution
    print("True Labels Distribution:", {0: true_labels.count(0), 1: true_labels.count(1)})
    print("Predictions Distribution:", {0: predictions.count(0), 1: predictions.count(1)})
    print(f"Successfully processed {len(true_labels)} examples")

    # Computing metrics
    from sklearn.metrics import accuracy_score, f1_score
    acc = accuracy_score(true_labels, predictions)
    f1 = f1_score(true_labels, predictions)

    return {"Accuracy": acc, "F1 Score": f1}

"""**Testing original Models (on IMDB):**"""

# DistilBERT
distilbert_results = evaluate_model(model_1, dataset, model_name="DistilBERT")
print("DistilBERT Results:", distilbert_results)

# BERT
bert_results = evaluate_model(model_2, dataset, model_name="BERT")
print("BERT Results:", bert_results)

# RoBERTa
roberta_results = evaluate_model(model_3, dataset, model_name="RoBERTa")
print("RoBERTa Results:", roberta_results)

"""**Evaluation of All Models on Yelp Dataset**"""

# DistilBERT on Yelp
distilbert_yelp_results = evaluate_model(model_1, yelp_binary_dataset, model_name="DistilBERT")
print("DistilBERT Results on Yelp:", distilbert_yelp_results)

# BERT on Yelp
bert_yelp_results = evaluate_model(model_2, yelp_binary_dataset, model_name="BERT")
print("BERT Results on Yelp:", bert_yelp_results)

# RoBERTa on Yelp
roberta_yelp_results = evaluate_model(model_3, yelp_binary_dataset, model_name="RoBERTa")
print("RoBERTa Results on Yelp:", roberta_yelp_results)

"""**Performance Metrics Comparison**

Based on the evaluation across both IMDB and Yelp datasets:

RoBERTa **performed best overall** with the highest metrics:

**IMDB: 82.8% accuracy, 0.828 F1 score**

**Yelp: 87.2% accuracy, 0.880 F1 score**

DistilBERT came in second:

**IMDB: 81.8% accuracy, 0.813 F1 score**

**Yelp: 86.4% accuracy, 0.865 F1 score**

BERT showed the l**owest performance**:

**IMDB: 76.8% accuracy, 0.784 F1 score**

**Yelp: 83.6% accuracy, 0.845 F1 score**

All models performed better on Yelp reviews than IMDB reviews, suggesting business reviews may be easier to classify than movie reviews.

**Error analysis:**

Finding 5 misclassified examples (using my own function):
"""

def find_misclassifications(model, dataset, model_name, num_examples=5):
    misclassified = []

    # Get a mix of positive and negative examples
    positive_indices = [i for i, example in enumerate(dataset["test"]) if example["label"] == 1]
    negative_indices = [i for i, example in enumerate(dataset["test"]) if example["label"] == 0]

    # Sample from both classes
    sampled_pos = random.sample(positive_indices, min(50, len(positive_indices)))
    sampled_neg = random.sample(negative_indices, min(50, len(negative_indices)))
    sampled_indices = sampled_pos + sampled_neg
    random.shuffle(sampled_indices)

    for i in sampled_indices:
        if len(misclassified) >= num_examples:
            break

        try:
            text = dataset["test"][i]["text"]
            text = text[:300]
            true_label = dataset["test"][i]["label"]

            pred = model(text, truncation=True, max_length=128)[0]

            # Handle different model output formatsagain
            if model_name == "DistilBERT":
                pred_label = 1 if pred["label"] == "POSITIVE" else 0
            elif model_name == "BERT":
                label_text = pred["label"]
                star_rating = int(label_text.split()[0])
                pred_label = 1 if star_rating >= 3 else 0
            elif model_name == "RoBERTa":
                label_text = pred["label"].lower()
                pred_label = 1 if label_text == "positive" else 0

            # if Misclassified
            if pred_label != true_label:
                misclassified.append({
                    "text": text,
                    "true_label": "positive" if true_label == 1 else "negative",
                    "predicted": "positive" if pred_label == 1 else "negative"
                })

        except Exception as e:
            # Skip if example causes errors
            continue

    return misclassified

# Run error analysis for each model on IMDB
print("ERROR ANALYSIS ON IMDB")
distilbert_errors = find_misclassifications(model_1, dataset, "DistilBERT")
bert_errors = find_misclassifications(model_2, dataset, "BERT")
roberta_errors = find_misclassifications(model_3, dataset, "RoBERTa")

print("\nDistilBERT Misclassifications:")
for i, error in enumerate(distilbert_errors):
    print(f"\nExample {i+1}:")
    print(f"Text: {error['text'][:100]}...")
    print(f"True: {error['true_label']}, Predicted: {error['predicted']}")

print("\nBERT Misclassifications:")
for i, error in enumerate(bert_errors):
    print(f"\nExample {i+1}:")
    print(f"Text: {error['text'][:100]}...")
    print(f"True: {error['true_label']}, Predicted: {error['predicted']}")

print("\nRoBERTa Misclassifications:")
for i, error in enumerate(roberta_errors):
    print(f"\nExample {i+1}:")
    print(f"Text: {error['text'][:100]}...")
    print(f"True: {error['true_label']}, Predicted: {error['predicted']}")

print("ERROR ANALYSIS ON YELP")
distilbert_yelp_errors = find_misclassifications(model_1, yelp_binary_dataset, "DistilBERT")
bert_yelp_errors = find_misclassifications(model_2, yelp_binary_dataset, "BERT")
roberta_yelp_errors = find_misclassifications(model_3, yelp_binary_dataset, "RoBERTa")

# Display sample Yelp errors
print("\nSample Yelp Review Misclassifications:")
for i, error in enumerate(distilbert_yelp_errors[:3]):
    print(f"\nExample {i+1}:")
    print(f"Text: {error['text'][:100]}...")
    print(f"True: {error['true_label']}, DistilBERT Predicted: {error['predicted']}")

"""**Analysis:**

Mixed sentiment reviews confused all models. Reviews containing both positive and negative elements (e.g., "The location is great... The food overall is very...") were often misclassified.

Sarcasm and irony were particularly challenging. For example, "This movie's only redeeming factor was the fact that it was on TV for free" was incorrectly classified as positive by RoBERTa.

**Visualization:**
"""

import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# Preparing results:
all_results = {
    "IMDB": {
        "DistilBERT": distilbert_results,
        "BERT": bert_results,
        "RoBERTa": roberta_results if 'roberta_results' in locals() else {"Accuracy": 0, "F1 Score": 0}
    },
    "Yelp": {
        "DistilBERT": distilbert_yelp_results if 'distilbert_yelp_results' in locals() else {"Accuracy": 0, "F1 Score": 0},
        "BERT": bert_yelp_results if 'bert_yelp_results' in locals() else {"Accuracy": 0, "F1 Score": 0},
        "RoBERTa": roberta_yelp_results if 'roberta_yelp_results' in locals() else {"Accuracy": 0, "F1 Score": 0}
    }
}

# Extracting model names and metrics
model_names = ["DistilBERT", "BERT", "RoBERTa"]
metrics = ["Accuracy", "F1 Score"]

# Csubplots
fig, axes = plt.subplots(2, 2, figsize=(15, 10))
fig.suptitle('Model Performance Comparison', fontsize=16)

# Bar width and positions
bar_width = 0.3
index = np.arange(len(model_names))

# Colors
colors = ['#3498db', '#e74c3c', '#2ecc71']

# IMDB Accuracy
imdb_acc = [all_results["IMDB"][model]["Accuracy"] for model in model_names]
axes[0, 0].bar(index, imdb_acc, bar_width, label='IMDB', color=colors)
axes[0, 0].set_xlabel('Models')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].set_title('IMDB Accuracy')
axes[0, 0].set_xticks(index)
axes[0, 0].set_xticklabels(model_names)
axes[0, 0].set_ylim(0, 1)
for i, v in enumerate(imdb_acc):
    axes[0, 0].text(i - 0.15, v + 0.02, f'{v:.3f}')

# Plot Yelp Accuracy
yelp_acc = [all_results["Yelp"][model]["Accuracy"] for model in model_names]
axes[0, 1].bar(index, yelp_acc, bar_width, label='Yelp', color=colors)
axes[0, 1].set_xlabel('Models')
axes[0, 1].set_ylabel('Accuracy')
axes[0, 1].set_title('Yelp Accuracy')
axes[0, 1].set_xticks(index)
axes[0, 1].set_xticklabels(model_names)
axes[0, 1].set_ylim(0, 1)
for i, v in enumerate(yelp_acc):
    axes[0, 1].text(i - 0.15, v + 0.02, f'{v:.3f}')

# Plot IMDB F1 Score
imdb_f1 = [all_results["IMDB"][model]["F1 Score"] for model in model_names]
axes[1, 0].bar(index, imdb_f1, bar_width, label='IMDB', color=colors)
axes[1, 0].set_xlabel('Models')
axes[1, 0].set_ylabel('F1 Score')
axes[1, 0].set_title('IMDB F1 Score')
axes[1, 0].set_xticks(index)
axes[1, 0].set_xticklabels(model_names)
axes[1, 0].set_ylim(0, 1)
for i, v in enumerate(imdb_f1):
    axes[1, 0].text(i - 0.15, v + 0.02, f'{v:.3f}')

# Plot Yelp F1 Score
yelp_f1 = [all_results["Yelp"][model]["F1 Score"] for model in model_names]
axes[1, 1].bar(index, yelp_f1, bar_width, label='Yelp', color=colors)
axes[1, 1].set_xlabel('Models')
axes[1, 1].set_ylabel('F1 Score')
axes[1, 1].set_title('Yelp F1 Score')
axes[1, 1].set_xticks(index)
axes[1, 1].set_xticklabels(model_names)
axes[1, 1].set_ylim(0, 1)
for i, v in enumerate(yelp_f1):
    axes[1, 1].text(i - 0.15, v + 0.02, f'{v:.3f}')

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

# side-by-side comparison across datasets
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
fig.suptitle('Dataset Comparison by Model', fontsize=16)

x = np.arange(len(model_names))
width = 0.35

# Plot accuracy comparison
ax1.bar(x - width/2, imdb_acc, width, label='IMDB')
ax1.bar(x + width/2, yelp_acc, width, label='Yelp')
ax1.set_ylabel('Accuracy')
ax1.set_title('Accuracy Comparison')
ax1.set_xticks(x)
ax1.set_xticklabels(model_names)
ax1.legend()
ax1.set_ylim(0, 1)

# Plot F1 comparison
ax2.bar(x - width/2, imdb_f1, width, label='IMDB')
ax2.bar(x + width/2, yelp_f1, width, label='Yelp')
ax2.set_ylabel('F1 Score')
ax2.set_title('F1 Score Comparison')
ax2.set_xticks(x)
ax2.set_xticklabels(model_names)
ax2.legend()
ax2.set_ylim(0, 1)

plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""**SUMMARY**

After comparing three sentiment analysis models on IMDB and Yelp datasets, RoBERTa performed best (82.8% accuracy on IMDB and 87.2% on Yelp), followed by DistilBERT (81.8%, 86.4%); BERT performed poorly).   All models performed better on Yelp business reviews than movie reviews, suggesting that shorter, more direct business feedback may be easier to categorize.   Analysis of misclassified examples showed that all models struggled with mixed reviews, misinterpreted sarcasm and irony, and missed the overall sentiment when "not a GREAT movie" was specified.   DistilBERT's small size and impressive performance surprised.  RoBERTa's text length sensitivity was a challenge that required special handling to avoid dimension mismatch errors during implementation.  Finally, fine-tuning models on domain-specific data, emphasizing concluding statements in preprocessing to better handle mixed sentiment, and developing specialized negation and sarcasm detection handling could improve performance even more.
"""